{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, LSTM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import pickle\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing the raw data to a .csv file-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The data is given in the form of .txt files. Here we convert the data into a .csv file \n",
    "# # and save it under the 'train' and 'test' directories for further use. Also the data here is \n",
    "# # arranged according to the subject ID. Therefore to break any correlation that may exists \n",
    "# # between the datapoints, we shuffle the dataset before dividing it into public and private \n",
    "# # datasets to be used in the Neural Network model.\n",
    "# features = []\n",
    "# with open(\"./UCI HAR Dataset/features.txt\") as file:\n",
    "#     for line in file:\n",
    "#         features.append(line.split()[1])\n",
    "        \n",
    "# # Renaming duplicate column names\n",
    "# names = []\n",
    "# count = {}\n",
    "# for feature in features:\n",
    "#     if(features.count(feature) > 1):\n",
    "#         names.append(feature)\n",
    "# for name in names:\n",
    "#     count[name] = features.count(name)\n",
    "\n",
    "# for i in range(len(features)):\n",
    "#     if(features[i] in names):\n",
    "#         num = count[features[i]]\n",
    "#         count[features[i]] -= 1;\n",
    "#         features[i] = str(features[i] + str(num))\n",
    "        \n",
    "# train_df = pd.read_csv(\"./UCI HAR Dataset/train/X_train.txt\", delim_whitespace = True,names= features)\n",
    "# train_df['subject_id'] = pd.read_csv(\"./UCI HAR Dataset/train/subject_train.txt\",header= None,squeeze=True)\n",
    "# train_df[\"activity\"] = pd.read_csv(\"./UCI HAR Dataset/train/y_train.txt\", header = None, squeeze = True)\n",
    "# activity = pd.read_csv(\"./UCI HAR Dataset/train/y_train.txt\", header = None, squeeze = True)\n",
    "# label_name = activity.map({1: \"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LYING\"})\n",
    "# train_df[\"activity_name\"] = label_name\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(\"./UCI HAR Dataset/test/X_test.txt\", delim_whitespace = True, names = features)\n",
    "# test_df['subject_id'] = pd.read_csv(\"./UCI HAR Dataset/test/subject_test.txt\",header= None,squeeze=True)\n",
    "# test_df[\"activity\"] = pd.read_csv(\"./UCI HAR Dataset/test/y_test.txt\", header = None, squeeze = True)\n",
    "# activity = pd.read_csv(\"./UCI HAR Dataset/test/y_test.txt\", header = None, squeeze = True)\n",
    "# label_name = activity.map({1: \"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LYING\"})\n",
    "# test_df[\"activity_name\"] = label_name\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving train and test data to a resuable .csv file-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = shuffle(train_df)\n",
    "# test_df = shuffle(test_df)\n",
    "# train_df.to_csv(\"./UCI HAR Dataset/train/train.csv\", index = False)\n",
    "# test_df.to_csv(\"./UCI HAR Dataset/test/test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data from the .csv file-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./UCI HAR Dataset/train/train.csv')\n",
    "test_df = pd.read_csv('./UCI HAR Dataset/test/test.csv')\n",
    "train_df['activity'] -= 1;\n",
    "test_df['activity'] -= 1;\n",
    "\n",
    "y_train = train_df['activity']\n",
    "X_train = train_df.drop(['activity','activity_name','subject_id'],axis=1)\n",
    "y_test = test_df['activity']\n",
    "X_test = test_df.drop(['activity','activity_name','subject_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train,X_test], axis = 0)\n",
    "# y_train = pd.concat([y_train,y_test], axis = 0)\n",
    "\n",
    "# X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(clf, trainX, trainY ,X, y_true): \n",
    "    clf.fit(trainX, trainY.values.ravel())\n",
    "    \n",
    "    X_test_values = X\n",
    "    y_predicted = clf.predict(X_test_values)\n",
    "#     print(classification_report(y_true, y_predicted))\n",
    "\n",
    "    array = confusion_matrix(y_true, y_predicted)\n",
    "    df_cm = pd.DataFrame(array, index = [i for i in LABELS],\n",
    "                    columns = [i for i in LABELS])\n",
    "#     plt.figure(figsize = (10,7))\n",
    "#     sn.heatmap(df_cm, annot=True, cmap=\"BuPu\",fmt='g')\n",
    "    \n",
    "   \n",
    "    recall = metrics.recall_score(y_true, y_predicted, average='macro' )\n",
    "    precision = metrics.precision_score(y_true, y_predicted, average='macro' )\n",
    "    f_score = metrics.f1_score(y_true, y_predicted, average=\"macro\") \n",
    "    acc = metrics.accuracy_score(y_true, y_predicted)\n",
    "    \n",
    "    print('Recall is: ', round(recall,4) * 100)\n",
    "    print('Precision is:', round(precision, 4)*100)\n",
    "    print('FScore is:', round(f_score, 4)*100)\n",
    "    print('Accuracy is: ',round(acc, 4) *100, '\\n\\n')\n",
    "    \n",
    "    \n",
    "def models(trainX, trainY, testX, testY):\n",
    "    #SVM\n",
    "    print('Results of SVM')\n",
    "    svm = SVC(C = 10, gamma = 0.01, kernel = 'rbf')\n",
    "    results(svm, trainX, trainY ,testX, testY)\n",
    "    \n",
    "    #KNN\n",
    "    print('Results of KNN')\n",
    "    knn = KNeighborsClassifier(metric= 'manhattan', n_neighbors = 15, weights = 'distance')\n",
    "    results(knn, trainX, trainY ,testX, testY)\n",
    "    \n",
    "    #RF\n",
    "    print('Results of RandomForest')\n",
    "    rf = RandomForestClassifier(max_features= 'sqrt', min_samples_split= 6, n_estimators = 50)\n",
    "    results(rf, trainX, trainY ,testX, testY)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_train)\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "\n",
    "temp = []\n",
    "for  i in most_important:\n",
    "    if i not in temp:\n",
    "        temp.append(i)\n",
    "        \n",
    "\n",
    "most_important = temp\n",
    "\n",
    "most_important = most_important[:250]\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('HAR FEATURE Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "# for i in range(len(ratio)):\n",
    "#     print(i, ratio[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train.iloc[:,most_important]\n",
    "X_test_pca = X_test.iloc[:,most_important]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(X_train_pca.columns))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_important_feature.txt', 'w') as file:\n",
    "    for i in most_important:\n",
    "        file.writelines([str(i),'\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature_name.txt', 'w') as file:\n",
    "    for i in sorted(X_train_pca.columns):\n",
    "        file.writelines([str(i),'\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in sorted(X_train_pca.columns):\n",
    "#     print(i.split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(most_important))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(X_train_pca, y_train ,X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_param_selection(X_train_pca, y_train, 5)\n",
    "\n",
    "# import pickle\n",
    "# import sys\n",
    "\n",
    "# p = pickle.dumps(svm)\n",
    "# print(sys.getsizeof(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_param_selection(X_train_pca, y_train, 5)\n",
    "# rf_param_selection(X_train_pca, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=None)\n",
    "\n",
    "lda = lda.fit(X_train_pca, y_train.values.ravel())\n",
    "\n",
    "X_train_lda = lda.transform(X_train_pca)   \n",
    "X_test_lda = lda.transform(X_test_pca) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.scalings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_var_ratios = lda.explained_variance_ratio_\n",
    "lda_var_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "models(X_train_lda, y_train, X_test_lda, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LDA().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "X_train_l = l.transform(X_train)   \n",
    "X_test_l = l.transform(X_test) \n",
    "\n",
    "X_train_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(X_train_l, y_train, X_test_l, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(X, y, n):\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    model = CREATE_MODEL(X.shape[1])\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    model = COMPILE(model)\n",
    "    history, model = FIT(model, X, y, n)\n",
    "\n",
    "    PLOT(history, n)\n",
    "\n",
    "    print(history.history['accuracy'])\n",
    "    print(max(history.history['accuracy']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def data(X, Y, path):\n",
    "#     path = 'Real-Time'\n",
    "    \n",
    "    test_x = pd.read_csv('{0}/{1}'.format(path, X))\n",
    "    test_y = pd.read_csv('{0}/{1}'.format(path, Y))\n",
    "    \n",
    "    names = test_x.columns\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range = [-1,1])\n",
    "    test_x  = scaler.fit_transform(test_x)\n",
    "        \n",
    "    return pd.DataFrame(test_x, columns = names), np.array(test_y)\n",
    "\n",
    "\n",
    "\n",
    "def check(clf, X, y, X_test, y_test):\n",
    "    clf.fit(X, y.ravel())\n",
    "        \n",
    "    y_predicted = clf.predict(X_test)\n",
    "    \n",
    "    recall = metrics.recall_score(y_test, y_predicted, average='macro' )\n",
    "    precision = metrics.precision_score(y_test, y_predicted, average='macro' )\n",
    "    f_score = metrics.f1_score(y_test, y_predicted, average=\"macro\") \n",
    "    acc = metrics.accuracy_score(y_test, y_predicted)\n",
    "\n",
    "    print(round(recall,4) * 100, 'Recall')\n",
    "    print(round(precision, 4)*100, 'pre')\n",
    "    print(round(f_score, 4)*100, \"fscore\")\n",
    "    print('Accuracy is: ', acc)\n",
    "    \n",
    "    array = confusion_matrix(y_test,y_predicted)\n",
    "    \n",
    "    df_cm = pd.DataFrame(array, index = [i for i in LABELS],\n",
    "                    columns = [i for i in LABELS])\n",
    "    \n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"BuPu\",fmt='g')\n",
    "    \n",
    "    return acc\n",
    "    \n",
    "    \n",
    "    \n",
    "def saveModel(filename, model): \n",
    "    filename = 'Models/{0}.sav'.format(filename)\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PCA\n",
    "# X_train_cnn = np.reshape(X_train_pca.values, (X_train_pca.values.shape[0], 1, X_train_pca.values.shape[1]))\n",
    "# X_test_cnn = np.reshape(X_test_pca.values, (X_test_pca.values.shape[0], 1, X_test_pca.values.shape[1]))\n",
    "\n",
    "#PCA+LDA\n",
    "\n",
    "# X_train_cnn = np.reshape(X_train_lda, (X_train_lda.shape[0], 1, X_train_lda.shape[1]))\n",
    "# X_test_cnn = np.reshape(X_test_lda, (X_test_lda.shape[0], 1, X_test_lda.shape[1]))\n",
    "\n",
    "#LDA\n",
    "\n",
    "# X_train_cnn = np.reshape(X_train_l, (X_train_l.shape[0], 1, X_train_l.shape[1]))\n",
    "# X_test_cnn = np.reshape(X_test_l, (X_test_l.shape[0], 1, X_test_l.shape[1]))\n",
    "\n",
    "\n",
    "#BASE DATASET\n",
    "X_train_cnn = np.reshape(X_train.values, (X_train.values.shape[0], 1, X_train.values.shape[1]))\n",
    "X_test_cnn = np.reshape(X_test.values, (X_test.values.shape[0], 1, X_test.values.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 1, 561)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1, 64)             107776    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1, 64)             12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 124,678\n",
      "Trainable params: 124,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "0.9835062\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D ,BatchNormalization, Activation,MaxPooling1D\n",
    "\n",
    "\n",
    "def CNN_MODEL_TRANSFER():\n",
    "    verbose, epochs, batch_size = 0, 40, 32\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding='same',  input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2] )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "     \n",
    "    history = model.fit(\n",
    "    X_train_cnn,\n",
    "    y_train.values,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint('./model_cnn.h5', save_best_only=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=0\n",
    "    )\n",
    "\n",
    "    print(max(history.history['accuracy']))\n",
    "\n",
    "    return model\n",
    " \n",
    "model_cnn = CNN_MODEL_TRANSFER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn_1 = tf.keras.models.load_model('model_cnn.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_cnn_1) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_BASE.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1, 64)             107776    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1, 64)             12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 248,966\n",
      "Trainable params: 124,678\n",
      "Non-trainable params: 124,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn_1 = tf.keras.models.load_model('model_cnn.h5')\n",
    "# model_cnn_1 = tf.keras.models.load_model('Models/PCA/model_cnn.h5')\n",
    "\n",
    "# model_cnn_1 = model_cnn\n",
    "model_cnn_1.pop()\n",
    "model_cnn_1.trainable = False\n",
    "\n",
    "model_cnn_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_2 = tf.keras.models.load_model('model_cnn.h5')\n",
    "# model_cnn_2 = tf.keras.models.load_model('Models/PCA/model_cnn.h5')\n",
    "\n",
    "# model_cnn_1 = model_cnn\n",
    "model_cnn_2.pop()\n",
    "model_cnn_2.pop()\n",
    "\n",
    "model_cnn_2.trainable = False\n",
    "\n",
    "model_cnn_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_real, y_test_real = data('X.csv', 'Y.csv', 'Real-Time')\n",
    "X, y = data('X.csv', 'Y.csv', 'real-time_v2')\n",
    "y -= 1\n",
    "\n",
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# CNN + PCA\n",
    "# X_train_real = X_train_real.iloc[:,most_important]\n",
    "# X_test_real = X_test_real.iloc[:,most_important]\n",
    "\n",
    "\n",
    "# CNN + (PCA + LDA)\n",
    "# X_train_real = lda.transform(X_train_real.iloc[:,most_important])\n",
    "# X_test_real = lda.transform(X_test_real.iloc[:,most_important])\n",
    "\n",
    "\n",
    "# CNN +  LDA\n",
    "# X_train_real = l.transform(X_train_real)\n",
    "# X_test_real = l.transform(X_test_real)\n",
    "\n",
    "# X_train_real = np.reshape(X_train_real, (X_train_real.shape[0], 1, X_train_real.shape[1]))\n",
    "# X_test_real = np.reshape(X_test_real, (X_test_real.shape[0], 1, X_test_real.shape[1]))\n",
    "\n",
    "\n",
    "# CNN Base Model  !! Comment PCA and PCA+LDA part\n",
    "X_train_real = np.reshape(X_train_real.values, (X_train_real.values.shape[0], 1, X_train_real.values.shape[1]))\n",
    "X_test_real = np.reshape(X_test_real.values, (X_test_real.values.shape[0], 1, X_test_real.values.shape[1]))\n",
    "\n",
    "\n",
    "# 1 Scenario \n",
    "X_train_real = model_cnn_1.predict(X_train_real)\n",
    "X_test_real = model_cnn_1.predict(X_test_real)\n",
    "\n",
    "# 2 Scenario \n",
    "# X_train_real = model_cnn_2.predict(X_train_real)\n",
    "# X_test_real = model_cnn_2.predict(X_test_real)\n",
    "\n",
    "\n",
    "\n",
    "X_train_real.shape, X_test_real.shape, y_train_real.shape, y_test_real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C = 10, gamma = 0.01, kernel= 'rbf')\n",
    "acc_svm = check(svm, X_train_real, y_train_real, X_test_real, y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'Base2'\n",
    "# saveModel('{0}/svm_{1}_{2}'.format(name, name, str( round(acc_svm, 3) )), svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C = 0.001, gamma = 1, kernel= 'poly')\n",
    "acc_svm = check(svm, X_train_real, y_train_real, X_test_real, y_test_real)\n",
    "\n",
    "# saveModel('BASE/svm_LDA_68.1', svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(X_train_real, y_train_real, nfolds):\n",
    "\n",
    "    param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1], \n",
    "                  'gamma' : [0.01, 0.1, 1], \n",
    "                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid'] }\n",
    "\n",
    "\n",
    "    grid_search = GridSearchCV( SVC(), \n",
    "                             param_grid, \n",
    "                             cv = nfolds,\n",
    "                             n_jobs = -1) # use all processor\n",
    "    grid_search.fit(X, y.ravel())\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "svc_param_selection(X_train_real, y_train_real, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(metric= 'manhattan', n_neighbors = 5, weights = 'distance')\n",
    "acc_knn = check(knn, X_train_real, y_train_real, X_test_real, y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveModel('{0}/knn_{1}_{2}'.format(name, name, str( round(acc_knn, 3) )), knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_param_selection(X, y, nfolds):\n",
    "\n",
    "    param_grid = {'n_neighbors' : [3,5,7,11,13,15,19,21],\n",
    "                'weights' : ['uniform', 'distance'], \n",
    "                'metric' : ['euclidean','manhattan'] }\n",
    "\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), \n",
    "                             param_grid, \n",
    "                             cv = nfolds,\n",
    "                             n_jobs = -1)\n",
    "\n",
    "    grid_search.fit(X,y)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "                  \n",
    "# knn_param_selection(X, y.ravel(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_features= 'sqrt', min_samples_split= 6, n_estimators = 30)\n",
    "acc_rf = check(rf, X_train_real, y_train_real, X_test_real, y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveModel('{0}/rf_{1}_{2}'.format(name, name, str( round(acc_rf, 3) )), rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_features= 'sqrt', min_samples_split= 6, n_estimators = 40)\n",
    "check(rf, X_train_real, y_train_real, X_test_real, y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_param_selection(X, y, nfolds):\n",
    "    param_grid = {'n_estimators': [40, 50, 150],\n",
    "                'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "                'min_samples_split': [4, 6,8, 10]}\n",
    "\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(),\n",
    "                             param_grid,\n",
    "                             cv = nfolds,\n",
    "                             n_jobs = -1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "# knn_param_selection(X, y.ravel(), 5)\n",
    "\n",
    "# rf_param_selection(X, y.ravel(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = np.reshape(X_train_real, (X_train_real.shape[0], 1, X_train_real.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test_real, (X_test_real.shape[0], 1, X_test_real.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(trainX, trainY, testX, testY):\n",
    "    n_steps = 64\n",
    "    n_feature = 1\n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units= trainX.shape[0], input_shape=(n_feature, n_steps) ))\n",
    "    lstm.add(Dense(128, activation='relu'))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(64, activation='relu'))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(32, activation='relu'))\n",
    "    lstm.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    print(lstm.summary())\n",
    "\n",
    "    lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    lstm.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "    _, accuracy = lstm.evaluate(testX, testY, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    y_pred = lstm.predict(testX)\n",
    "    y_pred = np.argmax(y_pred,axis=1)\n",
    "    \n",
    "    recall = metrics.recall_score(testY, y_pred, average='macro' )\n",
    "    precision = metrics.precision_score(testY, y_pred, average='macro' )\n",
    "    f_score = metrics.f1_score(testY, y_pred, average=\"macro\") \n",
    "\n",
    "    print(round(recall,4) * 100, 'Recall')\n",
    "    print(round(precision, 4)*100, 'pre')\n",
    "    print(round(f_score, 4)*100, \"fscore\")\n",
    "    print('Accuracy is: ', metrics.accuracy_score(testY, y_pred))\n",
    "    \n",
    "    array = confusion_matrix(testY, y_pred)\n",
    "    \n",
    "    df_cm = pd.DataFrame(array, index = [i for i in LABELS],\n",
    "                    columns = [i for i in LABELS])\n",
    "    \n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"BuPu\",fmt='g')\n",
    "\n",
    "    return accuracy, lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, lstm = LSTM_Model(X_train_lstm, y_train_real, X_test_lstm, y_test_real )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "lstm.save('Models/{0}/lstm_{1}_{2}.h5'.format(name,name,str(round(accuracy,3) )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "new = keras.models.load_model('Models/Base2/lstm_Base2_0.7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = new.predict(X_test_lstm)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print('Accuracy is: ', metrics.accuracy_score(y_test_real, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = new.evaluate(X_test_lstm, y_test_real, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables-\n",
    "epochs = 30\n",
    "# batch_size = 64\n",
    "batch_size = 16\n",
    "num_classes = 6\n",
    "fig_size = (9,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CREATE_MODEL(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = input_shape,))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(num_classes,activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def COMPILE(model):\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    return model;\n",
    "    \n",
    "def FIT(model,X,y,i):\n",
    "    history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(f'./model_{i}.h5', save_best_only=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=0\n",
    "    )\n",
    "    return history,model\n",
    "\n",
    "def PLOT(history,i):\n",
    "    epochs_range = range(epochs)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(epochs_range,train_loss,label=\"Training Loss\")\n",
    "    plt.plot(epochs_range,val_loss,label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model {i} - Loss Over Time\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(epochs_range,train_acc,label=\"Training accuracy\")\n",
    "    plt.plot(epochs_range,val_acc,label=\"Validation accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model {i} - Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "def CONFUSION_MATRIX(y_test, y_pred):\n",
    "    confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.set(font_scale=1.5)\n",
    "    labels = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LYING\"]\n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(confusionMatrix, cmap = \"Blues\", annot = True, fmt = \".0f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Global Model Confusion Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(X_train, y_train, 4)\n",
    "\n",
    "# X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# print('Accuracy is: ',metrics.accuracy_score(y_pred, y_test))\n",
    "\n",
    "# CONFUSION_MATRIX(y_test,y_pred)\n",
    "\n",
    "\n",
    "# model.pop()\n",
    "# model = tf.keras.models.load_model('Models/Base/model_4.h5')\n",
    "# model.pop()\n",
    "# model.trainable = False\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_real, y_test_real = data('X.csv', 'Y.csv', 'Real-Time')\n",
    "X, y = data('X.csv', 'Y.csv', 'Real-time_v2')\n",
    "y -= 1\n",
    "\n",
    "# print(X.describe())\n",
    "\n",
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# #BASE MODEL\n",
    "# X_train_real = model.predict(X_train_real.values)\n",
    "# X_test_real = model.predict(X_test_real.values)\n",
    "\n",
    "# PCA\n",
    "X_train_real = X_train_real.iloc[:,most_important]\n",
    "X_test_real = X_test_real.iloc[:,most_important]\n",
    "\n",
    "X_train_real = model_dimension.predict(X_train_real.values)\n",
    "X_test_real = model_dimension.predict(X_test_real.values)\n",
    "\n",
    "# #LDA\n",
    "# X_train_real = lda.transform(X_train_real.iloc[:,most_important])\n",
    "# X_test_real = lda.transform(X_test_real.iloc[:,most_important])\n",
    "\n",
    "# X_train_real = model_lda.predict(X_train_real)\n",
    "# X_test_real = model_lda.predict(X_test_real)\n",
    "\n",
    "\n",
    "\n",
    "#CNN Base Model\n",
    "# X_train_real = np.reshape(X_train_real.values, (X_train_real.values.shape[0], 1, X_train_real.values.shape[1]))\n",
    "# X_test_real = np.reshape(X_test_real.values, (X_test_real.values.shape[0], 1, X_test_real.values.shape[1]))\n",
    "\n",
    "# X_train_real = model_cnn.predict(X_train_real)\n",
    "# X_test_real = model_cnn.predict(X_test_real)\n",
    "\n",
    "\n",
    "# # CNN + PCA\n",
    "# X_train_real = X_train_real.iloc[:,most_important]\n",
    "# X_test_real = X_test_real.iloc[:,most_important]\n",
    "\n",
    "# X_train_real = np.reshape(X_train_real.values, (X_train_real.values.shape[0], 1, X_train_real.values.shape[1]))\n",
    "# X_test_real = np.reshape(X_test_real.values, (X_test_real.values.shape[0], 1, X_test_real.values.shape[1] ))\n",
    "\n",
    "# X_train_real = model_cnn.predict(X_train_real)\n",
    "# X_test_real = model_cnn.predict(X_test_real)\n",
    "\n",
    "\n",
    "\n",
    "# CNN + LDA\n",
    "# X_train_real = lda.transform(X_train_real.iloc[:,most_important])\n",
    "# X_test_real = lda.transform(X_test_real.iloc[:,most_important])\n",
    "\n",
    "# X_train_real = np.reshape(X_train_real, (X_train_real.shape[0], 1, X_train_real.shape[1]))\n",
    "# X_test_real = np.reshape(X_test_real, (X_test_real.shape[0], 1, X_test_real.shape[1]))\n",
    "\n",
    "# X_train_real = model_cnn.predict(X_train_real)\n",
    "# X_test_real = model_cnn.predict(X_test_real)\n",
    "\n",
    "\n",
    "X_train_real.shape, X_test_real.shape, y_train_real.shape, y_test_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_MODEL_TRANSFER():\n",
    "    num_classes, batch_size, epochs = 6, 32 , 30\n",
    "    \n",
    "    model_dimension = Sequential()\n",
    "    model_dimension.add(Input(shape = X_train_pca.shape[1],))\n",
    "    model_dimension.add(Dense(128,activation='relu'))\n",
    "    model_dimension.add(Dropout(0.2))\n",
    "    model_dimension.add(Dense(64,activation='relu'))\n",
    "    # model_dimension.add(Dropout(0.5))\n",
    "    # model_dimension.add(Dense(32,activation='relu'))\n",
    "    model_dimension.add(Dense(num_classes,activation='softmax')) \n",
    "\n",
    "\n",
    "    model_dimension.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model_dimension.fit(\n",
    "    X_train_pca,\n",
    "    y_train.values,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(f'./model_{10}.h5', save_best_only=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=0\n",
    "    )\n",
    "\n",
    "#     PLOT(history, 10)\n",
    "\n",
    "    print(history.history['accuracy'])\n",
    "    print(max(history.history['accuracy']))\n",
    "\n",
    "    return model_dimension\n",
    "\n",
    "model_dimension = PCA_MODEL_TRANSFER()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dimension = tf.keras.models.load_model('PCA/base_model.h5')\n",
    "# # model_dimension = tf.keras.models.load_model('model_10.h5')\n",
    "\n",
    "# model_dimension.summary()\n",
    "\n",
    "# model_dimension.count_params()\n",
    "\n",
    "\n",
    "# model_dimension = tf.keras.models.load_model('PCA/base_model.h5')\n",
    "model_dimension = tf.keras.models.load_model('model_10.h5')\n",
    "\n",
    "model_dimension.summary()\n",
    "model_dimension.pop()\n",
    "model_dimension.trainable = False\n",
    "\n",
    "model_dimension.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA_MODEL_TRANSFER(trainX, trainY):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape = trainX.shape[1],))\n",
    "#     model.add(Dense(12,activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(16,activation='relu'))\n",
    "# #     model_dimension.add(Dropout(0.5))\n",
    "# #     model_dimension.add(Dense(32,activation='relu'))\n",
    "#     model.add(Dense(num_classes,activation='softmax')) \n",
    "\n",
    "\n",
    "#     model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#     trainX,\n",
    "#     trainY,\n",
    "#     validation_split=0.2,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.ModelCheckpoint(f'./model_{12}.h5', save_best_only=True),\n",
    "#         tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#             monitor='val_loss',\n",
    "#             factor=0.1,\n",
    "#             patience=1\n",
    "#         )\n",
    "#     ],\n",
    "#     verbose=0\n",
    "#     )\n",
    "\n",
    "#     PLOT(history, 10)\n",
    "\n",
    "#     print(history.history['accuracy'])\n",
    "#     print(max(history.history['accuracy']))\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # model_lda = LDA_MODEL_TRANSFER(pd.DataFrame(X_train_lda), y_train)\n",
    "\n",
    "# model_lda = tf.keras.models.load_model('model_12.h5')\n",
    "\n",
    "\n",
    "# model_lda.pop()\n",
    "# model_lda.trainable = False\n",
    "\n",
    "# model_lda.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = model.predict(X_train)\n",
    "X_test_test = model.predict(X_test)\n",
    "\n",
    "X_train_test = np.reshape(X_train_test, (X_train_test.shape[0], 1, X_train_test.shape[1]))\n",
    "X_test_test = np.reshape(X_test_test, (X_test_test.shape[0], 1, X_test_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(X_train_real, y_train_real, 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = cnn.predict(X_test_real)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "recall = metrics.recall_score(y_test_real, y_pred, average='macro' )\n",
    "precision = metrics.precision_score(y_test_real, y_pred, average='macro' )\n",
    "f_score = metrics.f1_score(y_test_real, y_pred, average=\"macro\") \n",
    "\n",
    "print(round(recall,4) * 100, 'Recall')\n",
    "print(round(precision, 4)*100, 'pre')\n",
    "print(round(f_score, 4)*100, \"fscore\")\n",
    "print('Accuracy is: ', metrics.accuracy_score(y_test_real, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extracted by my Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = [-1,1])\n",
    "\n",
    "X_train_fex = pd.read_csv(\"TRAIN_NEW_ALL.csv\")\n",
    "X_test_fex = pd.read_csv(\"TEST_NEW_ALL.csv\")\n",
    "\n",
    "X_train_fex = scaler.fit_transform(X_train_fex)\n",
    "X_test_fex  = scaler.fit_transform(X_test_fex)\n",
    "\n",
    "X_train_fex = pd.DataFrame(X_train_fex)\n",
    "X_test_fex = pd.DataFrame(X_test_fex)\n",
    "\n",
    "print(X_train_fex.shape, X_test_fex.shape)\n",
    "\n",
    "\n",
    "y_train_fex = pd.read_csv(\"UCI HAR Dataset/train/y_train.txt\", delim_whitespace=True, header=None)[:-1]\n",
    "y_test_fex = pd.read_csv(\"UCI HAR Dataset/test/y_test.txt\", delim_whitespace=True, header=None)[:-1]\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "y_train_fex -= 1\n",
    "y_test_fex -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extracted = Model(X_train_fex, y_train_fex, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extracted.pop()\n",
    "model_extracted.trainable = True\n",
    "\n",
    "model_extracted.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data('X.csv', 'Y.csv', 'Real-time_v1')\n",
    "X = model_extracted.predict(X)\n",
    "\n",
    "svm = SVC(C = 10, gamma = 0.01, kernel= 'rbf')\n",
    "check(svm, X, y, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model replica number-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL REPLICA NO.1\n",
    "X_train_1 = X_train[0:2500]\n",
    "y_train_1 = y_train[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1 = PREPROCESS(X_train_1,y_train_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CREATE_MODEL(X_train_1.shape[1]);\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = COMPILE(model_1)\n",
    "history_1,model_1 = FIT(model_1,X_train_1,y_train_1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT(history_1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model replica number-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL REPLICA NO.2\n",
    "X_train_2 = X_train[2500:5000]\n",
    "y_train_2 = y_train[2500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, y_train_2 = PREPROCESS(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2 = CREATE_MODEL(X_train_2.shape[1])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = COMPILE(model_2)\n",
    "history_2,model_2 = FIT(model_2,X_train_2,y_train_2,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT(history_2,2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model replica number-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL REPLICA NO.3\n",
    "X_train_3 = X_train[5000:-1]\n",
    "y_train_3 = y_train[5000:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3, y_train_3 = PREPROCESS(X_train_3,y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = CREATE_MODEL(X_train_3.shape[1])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = COMPILE(model_3);\n",
    "history_3, model_3 = FIT(model_3,X_train_3,y_train_3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT(history_3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = CREATE_MODEL(X_test.shape[1])\n",
    "model_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "LOAD_MODELS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [max(history_1.history['accuracy']), max(history_2.history['accuracy']), max(history_3.history['accuracy'])]\n",
    "x = max(weights)\n",
    "idx = weights.index(x)\n",
    "weights[idx] = 1\n",
    "x = min(weights)\n",
    "idx = weights.index(x)\n",
    "weights[idx] = 0.02\n",
    "for i in range(3):\n",
    "    if(weights[i] != 1 and weights[i] != 0.02):\n",
    "        weights[i] = 0.03\n",
    "        break\n",
    "avg_model_weights = APPLY_WEIGHT_FUNCTION(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g.set_weights(avg_model_weights)\n",
    "model_g = COMPILE(model_g)\n",
    "_ , accuracy = model_g.evaluate(X_test,y_test,verbose=0)\n",
    "print(f\"Global Model Accuracy: {round(accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_g.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFUSION_MATRIX(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('PCA/most_important_feature.txt', header = None)\n",
    "test[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
